# ğŸ  House Price Prediction with PyTorch Deep Learning
## MLP (Multi-Layer Perceptron) Neural Network
## ğŸ¯ Project Overview

This project implements a **Multi-Layer Perceptron (MLP)** using PyTorch to predict house prices based on 80+ features.

**Key Highlights:**
- ğŸ“Š Deep Learning with PyTorch
- ğŸ§  4-layer Neural Network architecture
- ğŸ“ˆ RMSE optimization
- ğŸ“ Week 3 AI Engineering Assignment

---

## ğŸ“Š Results

| Metric | Score |
|--------|-------|
| **Kaggle RMSE** | **0,23174** |
| Validation RMSE | $37,101.24 |
| Train RMSE | $27,665.06 |
| Overfitting Ratio | 34.11% |

---

## ğŸ§  Model Architecture
```
Input Layer (80 features)
    â†“
Linear(80 â†’ 128) + ReLU + Dropout(0.2)
    â†“
Linear(128 â†’ 64) + ReLU + Dropout(0.2)
    â†“
Linear(64 â†’ 32) + ReLU
    â†“
Linear(32 â†’ 1)
    â†“
Output (SalePrice)
```

**Total Parameters:** ~20,737

---

## ğŸ”§ Tech Stack

- **PyTorch** - Deep Learning framework
- **Python 3.8+**
- **Pandas & NumPy** - Data manipulation
- **Scikit-learn** - Preprocessing (StandardScaler, LabelEncoder)
- **Matplotlib & Seaborn** - Visualization
- **Google Colab** - Development environment

---

## ğŸ“ Project Structure
```
house-prices-pytorch-deep-learning/
â”‚
â”œâ”€â”€ notebooks/           # Jupyter notebook with full pipeline
â”œâ”€â”€ models/              # Trained PyTorch model (.pth)
â”œâ”€â”€ submissions/         # Kaggle submission file
â”œâ”€â”€ screenshots/         # Training visualizations
â”œâ”€â”€ data/                # Dataset (or Kaggle link)
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install -r requirements.txt
```

### Run the Notebook

1. Clone this repository:
```bash
git clone https://github.com/[username]/house-prices-pytorch-deep-learning.git
cd house-prices-pytorch-deep-learning
```

2. Open the notebook:
```bash
jupyter notebook notebooks/House_Prices_PyTorch_MLP.ipynb
```

3. Run all cells

---

## ğŸ“ˆ Training Process

### Loss Curves

![Training Loss](screenshots/02_training_loss.png)

**Training Details:**
- **Epochs:** 100
- **Batch Size:** 32
- **Optimizer:** Adam (lr=0.001)
- **Loss Function:** MSELoss

**Observations:**
- Both train and validation losses decreased steadily
- Minimal overfitting (controlled with Dropout)
- Model converged after ~80 epochs

---

## ğŸ“Š Model Evaluation

### Predictions vs Actual

![Predictions vs Actual](screenshots/03_predictions_vs_actual.png)

### Residual Analysis

![Residuals](screenshots/04_residual_plot.png)

**Key Insights:**
- Strong correlation between predictions and actual values
- Residuals randomly distributed around zero
- Model performs well across all price ranges

---

## ğŸ” Data Preprocessing

**1. Missing Values:**
- Numerical: Median imputation
- Categorical: Most frequent imputation

**2. Encoding:**
- Label Encoding for categorical features (43 features)

**3. Scaling:**
- StandardScaler (mean=0, std=1) for all features

**4. Train-Validation Split:**
- 80% Training (1168 samples)
- 20% Validation (292 samples)

---

## ğŸ’¡ Key Learnings

### PyTorch Fundamentals:
- âœ… Building custom nn.Module models
- âœ… Training loops with forward/backward passes
- âœ… DataLoader and batch processing
- âœ… Device management (CPU/GPU)

### Deep Learning Concepts:
- âœ… Backpropagation and gradient descent
- âœ… Activation functions (ReLU)
- âœ… Regularization (Dropout)
- âœ… Loss functions (MSELoss)
- âœ… Optimizers (Adam)

### Model Evaluation:
- âœ… RMSE calculation
- âœ… Overfitting detection
- âœ… Loss curve analysis
- âœ… Residual analysis

---

## ğŸ¯ Comparison with ML Models

| Model | RMSE | Notes |
|-------|------|-------|
| Gradient Boosting (Week 2) | 0.1224 | Traditional ML |
| **PyTorch MLP (Week 3)** | **[SKORUN]** | Deep Learning |

**Insights:**
- [KarÅŸÄ±laÅŸtÄ±rma yapmak istersen ekle]

---

## ğŸ”® Future Improvements

**1. Architecture:**
- Try deeper networks (5-6 layers)
- Experiment with Batch Normalization
- Test different activation functions (LeakyReLU, ELU)

**2. Hyperparameters:**
- Learning rate scheduling
- Different optimizers (SGD with momentum, RMSprop)
- Varying dropout rates (0.1 - 0.5)

**3. Training:**
- Early stopping
- More epochs (200-300)
- Cross-validation

**4. Advanced Techniques:**
- Ensemble multiple models
- Feature engineering
- Data augmentation

---

## ğŸ“„ Assignment Requirements

This project fulfills the Week 3 Deep Learning assignment:

- âœ… PyTorch MLP implementation
- âœ… At least 2 hidden layers (4 layers total)
- âœ… ReLU activation functions
- âœ… MSELoss and Adam optimizer
- âœ… Train-Validation split (80-20)
- âœ… RMSE evaluation
- âœ… Kaggle submission
- âœ… Complete documentation

---

## ğŸ“ Presentation

**Key Points:**
1. Problem: House price prediction using Deep Learning
2. Architecture: 4-layer MLP with dropout
3. Training: 100 epochs, Adam optimizer
4. Results: Kaggle RMSE [SKORUN]
5. Learnings: PyTorch, backpropagation, overfitting control

---

## ğŸ‘¤ Author

**Mustafa Haybat**

- LinkedIn: [linkedin.com/in/mustafa-haybat](https://linkedin.com/in/mustafa-haybat)
- GitHub: [@mustafa-haybat](https://github.com/mustafa-haybat)

---

## ğŸ™ Acknowledgments

- Kaggle for the House Prices dataset
- PyTorch team for the excellent framework
- AI Engineering course instructors

---

## â­ Star this repo if you found it helpful!
```
